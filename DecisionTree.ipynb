{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of test case: 16281\n",
      "Train Time spent: 0.0740 seconds\n",
      "Test Time spent: 0.0018 seconds\n",
      "Overall Time spent: 0.0758 seconds\n",
      "train accuracy:  0.8656\n",
      "test accuracy:  0.8146\n",
      "[[11134  1301]\n",
      " [ 1717  2129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8664    0.8954    0.8806     12435\n",
      "        >50K     0.6207    0.5536    0.5852      3846\n",
      "\n",
      "    accuracy                         0.8146     16281\n",
      "   macro avg     0.7435    0.7245    0.7329     16281\n",
      "weighted avg     0.8084    0.8146    0.8109     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "adult=pd.read_csv('adult.data', names=headers, skipinitialspace=True)\n",
    "adultTest=pd.read_csv('updateAdult.test', names=headers, skipinitialspace=True)\n",
    "print(\"Total numer of test case: \" + str(len(adultTest)))\n",
    "\n",
    "def dataCleaningAndPreprocessing(dataset):\n",
    "    newDataSet = dataset\n",
    "    newDataSet=newDataSet.drop(['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week'],axis=1)\n",
    "    return newDataSet\n",
    "\n",
    "\n",
    "updateAdult = dataCleaningAndPreprocessing(adult)\n",
    "updateAdultTest = dataCleaningAndPreprocessing(adultTest)\n",
    "\n",
    "##########################\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_text\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x_train = updateAdult.drop('income', axis=1)\n",
    "y_train = updateAdult['income']\n",
    "x_test = updateAdultTest.drop('income', axis=1)\n",
    "y_test = updateAdultTest['income']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "startTime = time.time()\n",
    "x_train_encoded = encoder.fit_transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(x_train_encoded, y_train)\n",
    "trainEndTime = time.time()\n",
    "timeSpent = trainEndTime - startTime\n",
    "print(f\"Train Time spent: {timeSpent:.4f} seconds\")\n",
    "test_predictions_tree = dtc.predict(x_test_encoded)\n",
    "testEndTime = time.time()\n",
    "timeSpent = testEndTime - trainEndTime\n",
    "print(f\"Test Time spent: {timeSpent:.4f} seconds\")\n",
    "timeSpent = testEndTime - startTime\n",
    "print(f\"Overall Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "\n",
    "########################################\n",
    "train_predictions_tree = dtc.predict(x_train_encoded)\n",
    "train_accuracy_tree = accuracy_score(y_train, train_predictions_tree)\n",
    "test_accuracy_tree = accuracy_score(y_test, test_predictions_tree)\n",
    "print(f\"train accuracy:  {train_accuracy_tree:.4f}\")\n",
    "print(f\"test accuracy:  {test_accuracy_tree:.4f}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_test, test_predictions_tree)\n",
    "print(cm)\n",
    "print(classification_report(y_test, test_predictions_tree, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is DT with Data cleaning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Train Time spent: 0.0877 seconds\n",
      "Test Time spent: 0.0017 seconds\n",
      "Overall Time spent: 0.0894 seconds\n",
      "train accuracy:  0.9035\n",
      "test accuracy:  0.8206\n",
      "[[11250  1185]\n",
      " [ 1735  2111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8664    0.9047    0.8851     12435\n",
      "        >50K     0.6405    0.5489    0.5912      3846\n",
      "\n",
      "    accuracy                         0.8206     16281\n",
      "   macro avg     0.7534    0.7268    0.7381     16281\n",
      "weighted avg     0.8130    0.8206    0.8157     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "adult=pd.read_csv('adult.data', names=headers, skipinitialspace=True)\n",
    "adultTest=pd.read_csv('updateAdult.test', names=headers, skipinitialspace=True)\n",
    "\n",
    "def handleUnknownNativeCountry(country) :\n",
    "    if country == '?' :\n",
    "        return 'United-States'\n",
    "    else : \n",
    "        return country\n",
    "\n",
    "#data-preprocessing change continues to discrete\n",
    "#process age\n",
    "def convertAge(age):\n",
    "    if age < 22 :\n",
    "        return '<22'\n",
    "    elif age >=72 :\n",
    "        return '>=72'\n",
    "    else :\n",
    "        base_age = (age - 22) // 5\n",
    "        start_age = 22 + base_age * 5\n",
    "        end_age = 22 + 4 + base_age * 5\n",
    "        return f\"{start_age}-{end_age}\"\n",
    "\n",
    "#process capital-gain and capital-loss\n",
    "def convertCapital(capitalGain, capitalLoss):\n",
    "    dataLen = len(capitalGain)\n",
    "    if dataLen != len(capitalLoss):\n",
    "        raise AssertionError(\"Lengths of capitalGain and capitalLoss are not equal.\")\n",
    "    i = 0\n",
    "    capitalOptimized = []\n",
    "    while i < dataLen:\n",
    "        profit = capitalGain[i] - capitalLoss[i]\n",
    "        if(profit == 0):\n",
    "            capitalOptimized.append('No captial profit')\n",
    "        elif (profit < -3000):\n",
    "             capitalOptimized.append('<-3000')\n",
    "        elif (profit < 0):\n",
    "            capitalOptimized.append('>-3000&<0')\n",
    "        elif (profit <3000):\n",
    "            capitalOptimized.append('>0&<3000')\n",
    "        elif (profit < 6000):\n",
    "            capitalOptimized.append('>=3000&<6000')\n",
    "        else:\n",
    "            capitalOptimized.append('>=6000')\n",
    "        i = i + 1\n",
    "    return capitalOptimized\n",
    "\n",
    "# def combineHoursPerWeek(hoursPerWeek) :\n",
    "#     combineHoursPerWeekData = []\n",
    "#     for hoursPerWeekData in hoursPerWeek :\n",
    "#         hours = '=40'\n",
    "#         if hoursPerWeekData < 40 :\n",
    "#             hours = '<40'\n",
    "#         elif hoursPerWeekData > 40 :\n",
    "#             hours = '>40'\n",
    "#         combineHoursPerWeekData.append(hours)\n",
    "#     return combineHoursPerWeekData\n",
    "\n",
    "def combineHoursPerWeek(hoursPerWeek) :\n",
    "    combineHoursPerWeekData = []\n",
    "    for hoursPerWeekData in hoursPerWeek :\n",
    "        if hoursPerWeekData < 33:\n",
    "            hours = \"<33\"\n",
    "        elif hoursPerWeekData < 40 :\n",
    "            hours = '>=33 & <40'\n",
    "        elif hoursPerWeekData < 45 :\n",
    "            hours = \">=40 & <45\"\n",
    "        elif hoursPerWeekData < 52 :\n",
    "            hours = \">=45 & <52\"\n",
    "        else :\n",
    "            hours = '>=52'\n",
    "        combineHoursPerWeekData.append(hours)\n",
    "    return combineHoursPerWeekData\n",
    "\n",
    "countryGroups = {\n",
    "    \"Local\" : ['United-States','?', 'South'],\n",
    "    \"North America\": ['Canada', 'Outlying-US(Guam-USVI-etc)', 'Puerto-Rico', 'Mexico'],\n",
    "    \"Europe\": ['Holand-Netherlands', 'England', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Portugal', \n",
    "               'Germany', 'Greece', 'Italy', 'Poland', 'Hungary'],\n",
    "    \"Asia\": ['Laos', 'India', 'Philippines', 'Hong', 'Japan', 'Cambodia', 'China', 'Taiwan', 'Iran', 'Thailand', 'Vietnam'],\n",
    "    \"Other America\": ['Jamaica', 'Cuba', 'Dominican-Republic', 'Haiti', 'Trinadad&Tobago', 'Guatemala', 'Honduras', 'El-Salvador', 'Nicaragua','Ecuador', 'Columbia', 'Peru'],\n",
    "}\n",
    "def countryToArea(country):\n",
    "    for group, countries in countryGroups.items():\n",
    "        if country in countries:\n",
    "            return group\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "educationLevel = {\n",
    "    \"Primary\" : ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'],\n",
    "    \"Middle\" : ['HS-grad','Some-college'],\n",
    "    \"Associate\" : ['Assoc-voc', 'Assoc-acdm'],\n",
    "    \"Senior\": ['Bachelors', 'Masters'],\n",
    "    \"Professional\": ['Doctorate', 'Prof-school']\n",
    "} \n",
    "def convertToEducationLevel(education):\n",
    "    for level, education in educationLevel.items() :\n",
    "        if level in education :\n",
    "            return level\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "def dataCleaningAndPreprocessing(dataset, cleanMissingData):\n",
    "    newDataSet = dataset\n",
    "    newDataSet=newDataSet.drop(['fnlwgt','education-num','relationship'],axis=1) #axis = 1 means drop column, axis = 0 => drop row\n",
    "    newDataSet['age'] = newDataSet['age'].apply(convertAge)\n",
    "    newDataSet['capital-gain'] = convertCapital(newDataSet['capital-gain'], newDataSet['capital-loss'])\n",
    "    newDataSet=newDataSet.drop('capital-loss',axis=1)\n",
    "    newDataSet['hours-per-week'] = combineHoursPerWeek(newDataSet['hours-per-week'])\n",
    "    if cleanMissingData == True: \n",
    "        newDataSet = newDataSet.loc[ (newDataSet['workclass'] != '?') & (newDataSet['occupation'] != '?') & (newDataSet['native-country']!= '?')]\n",
    "    else :\n",
    "        # newDataSet['native-country'] = newDataSet['native-country'].apply(handleUnknownNativeCountry)\n",
    "        # tread '?' as a group in workclass and occupation, because may have some reasons such as confidential, indescribable\n",
    "        newDataSet['area'] = newDataSet['native-country'].apply(countryToArea)\n",
    "        newDataSet = newDataSet.drop('native-country', axis=1)\n",
    "    newDataSet['education'] = newDataSet['education'].apply(convertToEducationLevel)\n",
    "    return newDataSet\n",
    "\n",
    "def removeHeaders(headers):\n",
    "    headers.remove('fnlwgt') \n",
    "    headers.remove('education-num')\n",
    "    headers.remove('relationship')\n",
    "    headers.remove('capital-loss')\n",
    "    headers.remove('native-country')\n",
    "    return headers\n",
    "    \n",
    "updateAdult= dataCleaningAndPreprocessing(adult, False)\n",
    "updateAdultTest= dataCleaningAndPreprocessing(adultTest, False)\n",
    "headers = removeHeaders(headers)\n",
    "print(len(updateAdult.columns))\n",
    "\n",
    "########data-balancing for original data  ==> tested useless\n",
    "# from sklearn.utils import resample\n",
    "# majority = updateAdult[updateAdult.income == '<=50K'];\n",
    "# minority = updateAdult[updateAdult.income == '>50K'];\n",
    "# print(len(majority));\n",
    "# print(len(minority));\n",
    "\n",
    "# minority = resample(minority, replace=True, n_samples=len(majority))\n",
    "# updateAdult = pd.concat([majority, minority])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "x_train = updateAdult.drop('income', axis=1)\n",
    "y_train = updateAdult['income']\n",
    "x_test = updateAdultTest.drop('income', axis=1)\n",
    "y_test = updateAdultTest['income']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "startTime = time.time()\n",
    "x_train_encoded = encoder.fit_transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(x_train_encoded, y_train)\n",
    "trainEndTime = time.time()\n",
    "timeSpent = trainEndTime - startTime\n",
    "print(f\"Train Time spent: {timeSpent:.4f} seconds\")\n",
    "test_predictions_tree = dtc.predict(x_test_encoded)\n",
    "testEndTime = time.time()\n",
    "timeSpent = testEndTime - trainEndTime\n",
    "print(f\"Test Time spent: {timeSpent:.4f} seconds\")\n",
    "timeSpent = testEndTime - startTime\n",
    "print(f\"Overall Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "\n",
    "########################################\n",
    "train_predictions_tree = dtc.predict(x_train_encoded)\n",
    "train_accuracy_tree = accuracy_score(y_train, train_predictions_tree)\n",
    "test_accuracy_tree = accuracy_score(y_test, test_predictions_tree)\n",
    "print(f\"train accuracy:  {train_accuracy_tree:.4f}\")\n",
    "print(f\"test accuracy:  {test_accuracy_tree:.4f}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_test, test_predictions_tree)\n",
    "print(cm)\n",
    "print(classification_report(y_test, test_predictions_tree, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is DT with Data cleaning and more preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "get best other params {'max_depth': 15, 'min_samples_leaf': 9, 'min_samples_split': 5}\n",
      "Pre-Pruning Time spent: 6.6523 seconds\n",
      "get total number of 1643\n",
      "Best ccp_alpha: 8.26993609770552e-05\n",
      "Post-Pruning Time spent: 343.6835 seconds\n",
      "Train Time spent: 350.4382 seconds\n",
      "Test Time spent: 0.0009 seconds\n",
      "Overall Time spent: 350.4391 seconds\n",
      "train accuracy:  0.8602\n",
      "test accuracy:  0.8518\n",
      "[[11590   845]\n",
      " [ 1568  2278]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8808    0.9320    0.9057     12435\n",
      "        >50K     0.7294    0.5923    0.6538      3846\n",
      "\n",
      "    accuracy                         0.8518     16281\n",
      "   macro avg     0.8051    0.7622    0.7797     16281\n",
      "weighted avg     0.8451    0.8518    0.8462     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "adult=pd.read_csv('adult.data', names=headers, skipinitialspace=True)\n",
    "adultTest=pd.read_csv('updateAdult.test', names=headers, skipinitialspace=True)\n",
    "\n",
    "def handleUnknownNativeCountry(country) :\n",
    "    if country == '?' :\n",
    "        return 'United-States'\n",
    "    else : \n",
    "        return country\n",
    "\n",
    "#data-preprocessing change continues to discrete\n",
    "#process age\n",
    "def convertAge(age):\n",
    "    if age < 22 :\n",
    "        return '<22'\n",
    "    elif age >=72 :\n",
    "        return '>=72'\n",
    "    else :\n",
    "        base_age = (age - 22) // 5\n",
    "        start_age = 22 + base_age * 5\n",
    "        end_age = 22 + 4 + base_age * 5\n",
    "        return f\"{start_age}-{end_age}\"\n",
    "\n",
    "#process capital-gain and capital-loss\n",
    "def convertCapital(capitalGain, capitalLoss):\n",
    "    dataLen = len(capitalGain)\n",
    "    if dataLen != len(capitalLoss):\n",
    "        raise AssertionError(\"Lengths of capitalGain and capitalLoss are not equal.\")\n",
    "    i = 0\n",
    "    capitalOptimized = []\n",
    "    while i < dataLen:\n",
    "        profit = capitalGain[i] - capitalLoss[i]\n",
    "        if(profit == 0):\n",
    "            capitalOptimized.append('No captial profit')\n",
    "        elif (profit < -3000):\n",
    "             capitalOptimized.append('<-3000')\n",
    "        elif (profit < 0):\n",
    "            capitalOptimized.append('>-3000&<0')\n",
    "        elif (profit <3000):\n",
    "            capitalOptimized.append('>0&<3000')\n",
    "        elif (profit < 6000):\n",
    "            capitalOptimized.append('>=3000&<6000')\n",
    "        else:\n",
    "            capitalOptimized.append('>=6000')\n",
    "        i = i + 1\n",
    "    return capitalOptimized\n",
    "\n",
    "# def combineHoursPerWeek(hoursPerWeek) :\n",
    "#     combineHoursPerWeekData = []\n",
    "#     for hoursPerWeekData in hoursPerWeek :\n",
    "#         hours = '=40'\n",
    "#         if hoursPerWeekData < 40 :\n",
    "#             hours = '<40'\n",
    "#         elif hoursPerWeekData > 40 :\n",
    "#             hours = '>40'\n",
    "#         combineHoursPerWeekData.append(hours)\n",
    "#     return combineHoursPerWeekData\n",
    "\n",
    "def combineHoursPerWeek(hoursPerWeek) :\n",
    "    combineHoursPerWeekData = []\n",
    "    for hoursPerWeekData in hoursPerWeek :\n",
    "        if hoursPerWeekData < 33:\n",
    "            hours = \"<33\"\n",
    "        elif hoursPerWeekData < 40 :\n",
    "            hours = '>=33 & <40'\n",
    "        elif hoursPerWeekData < 45 :\n",
    "            hours = \">=40 & <45\"\n",
    "        elif hoursPerWeekData < 52 :\n",
    "            hours = \">=45 & <52\"\n",
    "        else :\n",
    "            hours = '>=52'\n",
    "        combineHoursPerWeekData.append(hours)\n",
    "    return combineHoursPerWeekData\n",
    "\n",
    "countryGroups = {\n",
    "    \"Local\" : ['United-States','?', 'South'],\n",
    "    \"North America\": ['Canada', 'Outlying-US(Guam-USVI-etc)', 'Puerto-Rico', 'Mexico'],\n",
    "    \"Europe\": ['Holand-Netherlands', 'England', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Portugal', \n",
    "               'Germany', 'Greece', 'Italy', 'Poland', 'Hungary'],\n",
    "    \"Asia\": ['Laos', 'India', 'Philippines', 'Hong', 'Japan', 'Cambodia', 'China', 'Taiwan', 'Iran', 'Thailand', 'Vietnam'],\n",
    "    \"Other America\": ['Jamaica', 'Cuba', 'Dominican-Republic', 'Haiti', 'Trinadad&Tobago', 'Guatemala', 'Honduras', 'El-Salvador', 'Nicaragua','Ecuador', 'Columbia', 'Peru'],\n",
    "}\n",
    "def countryToArea(country):\n",
    "    for group, countries in countryGroups.items():\n",
    "        if country in countries:\n",
    "            return group\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "educationLevel = {\n",
    "    \"Primary\" : ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'],\n",
    "    \"Middle\" : ['HS-grad','Some-college'],\n",
    "    \"Associate\" : ['Assoc-voc', 'Assoc-acdm'],\n",
    "    \"Senior\": ['Bachelors', 'Masters'],\n",
    "    \"Professional\": ['Doctorate', 'Prof-school']\n",
    "} \n",
    "def convertToEducationLevel(education):\n",
    "    for level, education in educationLevel.items() :\n",
    "        if level in education :\n",
    "            return level\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "def dataCleaningAndPreprocessing(dataset, cleanMissingData):\n",
    "    newDataSet = dataset\n",
    "    newDataSet=newDataSet.drop(['fnlwgt','education-num','relationship'],axis=1) #axis = 1 means drop column, axis = 0 => drop row\n",
    "    newDataSet['age'] = newDataSet['age'].apply(convertAge)\n",
    "    newDataSet['capital-gain'] = convertCapital(newDataSet['capital-gain'], newDataSet['capital-loss'])\n",
    "    newDataSet=newDataSet.drop('capital-loss',axis=1)\n",
    "    newDataSet['hours-per-week'] = combineHoursPerWeek(newDataSet['hours-per-week'])\n",
    "    if cleanMissingData == True: \n",
    "        newDataSet = newDataSet.loc[ (newDataSet['workclass'] != '?') & (newDataSet['occupation'] != '?') & (newDataSet['native-country']!= '?')]\n",
    "    else :\n",
    "        # newDataSet['native-country'] = newDataSet['native-country'].apply(handleUnknownNativeCountry)\n",
    "        # tread '?' as a group in workclass and occupation, because may have some reasons such as confidential, indescribable\n",
    "        newDataSet['area'] = newDataSet['native-country'].apply(countryToArea)\n",
    "        newDataSet = newDataSet.drop('native-country', axis=1)\n",
    "    # newDataSet['education'] = newDataSet['education'].apply(convertToEducationLevel)\n",
    "    return newDataSet\n",
    "\n",
    "def removeHeaders(headers):\n",
    "    headers.remove('fnlwgt') \n",
    "    headers.remove('education-num')\n",
    "    headers.remove('relationship')\n",
    "    headers.remove('capital-loss')\n",
    "    headers.remove('native-country')\n",
    "    return headers\n",
    "    \n",
    "updateAdult= dataCleaningAndPreprocessing(adult, False)\n",
    "updateAdultTest= dataCleaningAndPreprocessing(adultTest, False)\n",
    "headers = removeHeaders(headers)\n",
    "print(len(updateAdult.columns))\n",
    "\n",
    "########data-balancing for original data  ==> tested useless\n",
    "# from sklearn.utils import resample\n",
    "# majority = updateAdult[updateAdult.income == '<=50K'];\n",
    "# minority = updateAdult[updateAdult.income == '>50K'];\n",
    "# print(len(majority));\n",
    "# print(len(minority));\n",
    "\n",
    "# minority = resample(minority, replace=True, n_samples=len(majority))\n",
    "# updateAdult = pd.concat([majority, minority])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_text\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x_train = updateAdult.drop('income', axis=1)\n",
    "y_train = updateAdult['income']\n",
    "x_test = updateAdultTest.drop('income', axis=1)\n",
    "y_test = updateAdultTest['income']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "startTime = time.time()\n",
    "x_train_encoded = encoder.fit_transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "\n",
    "#get Best criterion, max_depth, min_samples_leaf, and min_samples_split\n",
    "prepruningStartTime = time.time()\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    # 'criterion': ['gini', \"entropy\"],\n",
    "    'max_depth': [8,9,10,11,13,15],\n",
    "    'min_samples_split': range(5,20,4), \n",
    "    'min_samples_leaf': range(3,10,1)\n",
    "}\n",
    "gridSearch = GridSearchCV(dtc, \n",
    "                           param_grid,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1,\n",
    "                           cv=5)\n",
    "gridSearch.fit(x_train_encoded, y_train)\n",
    "best_params = gridSearch.best_params_\n",
    "print(f'get best other params {best_params}')\n",
    "prepruningEndTime = time.time()\n",
    "timeSpent = prepruningEndTime - prepruningStartTime\n",
    "print(f\"Pre-Pruning Time spent: {timeSpent:.4f} seconds\")\n",
    "#get best ccp_alpha\n",
    "\n",
    "\n",
    "path = dtc.cost_complexity_pruning_path(x_train_encoded, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "ccp_alphas = [alpha for alpha in ccp_alphas if alpha >= 0]\n",
    "ccp_alphas = list(set(ccp_alphas)) #remove duplicate values.\n",
    "print(f'get total number of {len(ccp_alphas)}')\n",
    "# Cross-validation for each alpha\n",
    "best_ccp_alpha = 0.0001\n",
    "scores = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dtc = DecisionTreeClassifier(\n",
    "                                # criterion=best_params.get('criterion'),\n",
    "                                max_depth=best_params.get('max_depth'), \n",
    "                                min_samples_leaf=best_params.get('min_samples_leaf'), \n",
    "                                min_samples_split=best_params.get('min_samples_split'),\n",
    "                                ccp_alpha=ccp_alpha)\n",
    "    score = np.median(cross_val_score(dtc, \n",
    "                                    x_train_encoded, \n",
    "                                    y_train,\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=0,\n",
    "                                    cv=5))\n",
    "    scores.append(score)\n",
    "best_ccp_alpha = ccp_alphas[np.argmax(scores)]\n",
    "print(\"Best ccp_alpha:\", best_ccp_alpha)\n",
    "ccpAlphaEndtime = time.time()\n",
    "timeSpent = ccpAlphaEndtime - prepruningEndTime\n",
    "print(f\"Post-Pruning Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "dtc = DecisionTreeClassifier(\n",
    "                                # criterion=best_params.get('criterion'),\n",
    "                                max_depth=best_params.get('max_depth'), \n",
    "                                min_samples_leaf=best_params.get('min_samples_leaf'), \n",
    "                                min_samples_split=best_params.get('min_samples_split'),\n",
    "                                ccp_alpha=best_ccp_alpha\n",
    "                            )\n",
    "dtc.fit(x_train_encoded, y_train)\n",
    "# r = export_text(dtc, feature_names=headers)\n",
    "# print(r) #print tree\n",
    "trainEndtime = time.time()\n",
    "timeSpent = trainEndtime - startTime\n",
    "print(f\"Train Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "test_predictions_tree = dtc.predict(x_test_encoded)\n",
    "preditEndtime = time.time()\n",
    "timeSpent = preditEndtime - trainEndtime\n",
    "print(f\"Test Time spent: {timeSpent:.4f} seconds\")\n",
    "timeSpent = preditEndtime - startTime\n",
    "print(f\"Overall Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "train_predictions_tree = dtc.predict(x_train_encoded)\n",
    "train_accuracy_tree = accuracy_score(y_train, train_predictions_tree)\n",
    "test_accuracy_tree = accuracy_score(y_test, test_predictions_tree)\n",
    "print(f\"train accuracy:  {train_accuracy_tree:.4f}\")\n",
    "print(f\"test accuracy:  {test_accuracy_tree:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_test, test_predictions_tree)\n",
    "#cm[0][0] = TP\n",
    "#cm[1][1] = TN\n",
    "#cm[0][1] = FP\n",
    "#cm[1][0] = FN\n",
    "print(cm)\n",
    "print(classification_report(y_test, test_predictions_tree, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is final optimized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "get total number of 1643\n",
      "Best ccp_alpha: 8.707372212244935e-05\n",
      "Post-Pruning Time spent: 377.3860 seconds\n",
      "Train Time spent: 377.5291 seconds\n",
      "Test Time spent: 0.0011 seconds\n",
      "Overall Time spent: 377.5302 seconds\n",
      "train accuracy:  0.8614\n",
      "test accuracy:  0.8499\n",
      "[[11593   842]\n",
      " [ 1601  2245]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8787    0.9323    0.9047     12435\n",
      "        >50K     0.7272    0.5837    0.6476      3846\n",
      "\n",
      "    accuracy                         0.8499     16281\n",
      "   macro avg     0.8030    0.7580    0.7762     16281\n",
      "weighted avg     0.8429    0.8499    0.8440     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "adult=pd.read_csv('adult.data', names=headers, skipinitialspace=True)\n",
    "adultTest=pd.read_csv('updateAdult.test', names=headers, skipinitialspace=True)\n",
    "\n",
    "def handleUnknownNativeCountry(country) :\n",
    "    if country == '?' :\n",
    "        return 'United-States'\n",
    "    else : \n",
    "        return country\n",
    "\n",
    "#data-preprocessing change continues to discrete\n",
    "#process age\n",
    "def convertAge(age):\n",
    "    if age < 22 :\n",
    "        return '<22'\n",
    "    elif age >=72 :\n",
    "        return '>=72'\n",
    "    else :\n",
    "        base_age = (age - 22) // 5\n",
    "        start_age = 22 + base_age * 5\n",
    "        end_age = 22 + 4 + base_age * 5\n",
    "        return f\"{start_age}-{end_age}\"\n",
    "\n",
    "#process capital-gain and capital-loss\n",
    "def convertCapital(capitalGain, capitalLoss):\n",
    "    dataLen = len(capitalGain)\n",
    "    if dataLen != len(capitalLoss):\n",
    "        raise AssertionError(\"Lengths of capitalGain and capitalLoss are not equal.\")\n",
    "    i = 0\n",
    "    capitalOptimized = []\n",
    "    while i < dataLen:\n",
    "        profit = capitalGain[i] - capitalLoss[i]\n",
    "        if(profit == 0):\n",
    "            capitalOptimized.append('No captial profit')\n",
    "        elif (profit < -3000):\n",
    "             capitalOptimized.append('<-3000')\n",
    "        elif (profit < 0):\n",
    "            capitalOptimized.append('>-3000&<0')\n",
    "        elif (profit <3000):\n",
    "            capitalOptimized.append('>0&<3000')\n",
    "        elif (profit < 6000):\n",
    "            capitalOptimized.append('>=3000&<6000')\n",
    "        else:\n",
    "            capitalOptimized.append('>=6000')\n",
    "        i = i + 1\n",
    "    return capitalOptimized\n",
    "\n",
    "# def combineHoursPerWeek(hoursPerWeek) :\n",
    "#     combineHoursPerWeekData = []\n",
    "#     for hoursPerWeekData in hoursPerWeek :\n",
    "#         hours = '=40'\n",
    "#         if hoursPerWeekData < 40 :\n",
    "#             hours = '<40'\n",
    "#         elif hoursPerWeekData > 40 :\n",
    "#             hours = '>40'\n",
    "#         combineHoursPerWeekData.append(hours)\n",
    "#     return combineHoursPerWeekData\n",
    "\n",
    "def combineHoursPerWeek(hoursPerWeek) :\n",
    "    combineHoursPerWeekData = []\n",
    "    for hoursPerWeekData in hoursPerWeek :\n",
    "        if hoursPerWeekData < 33:\n",
    "            hours = \"<33\"\n",
    "        elif hoursPerWeekData < 40 :\n",
    "            hours = '>=33 & <40'\n",
    "        elif hoursPerWeekData < 45 :\n",
    "            hours = \">=40 & <45\"\n",
    "        elif hoursPerWeekData < 52 :\n",
    "            hours = \">=45 & <52\"\n",
    "        else :\n",
    "            hours = '>=52'\n",
    "        combineHoursPerWeekData.append(hours)\n",
    "    return combineHoursPerWeekData\n",
    "\n",
    "countryGroups = {\n",
    "    \"Local\" : ['United-States','?', 'South'],\n",
    "    \"North America\": ['Canada', 'Outlying-US(Guam-USVI-etc)', 'Puerto-Rico', 'Mexico'],\n",
    "    \"Europe\": ['Holand-Netherlands', 'England', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Portugal', \n",
    "               'Germany', 'Greece', 'Italy', 'Poland', 'Hungary'],\n",
    "    \"Asia\": ['Laos', 'India', 'Philippines', 'Hong', 'Japan', 'Cambodia', 'China', 'Taiwan', 'Iran', 'Thailand', 'Vietnam'],\n",
    "    \"Other America\": ['Jamaica', 'Cuba', 'Dominican-Republic', 'Haiti', 'Trinadad&Tobago', 'Guatemala', 'Honduras', 'El-Salvador', 'Nicaragua','Ecuador', 'Columbia', 'Peru'],\n",
    "}\n",
    "def countryToArea(country):\n",
    "    for group, countries in countryGroups.items():\n",
    "        if country in countries:\n",
    "            return group\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "educationLevel = {\n",
    "    \"Primary\" : ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'],\n",
    "    \"Middle\" : ['HS-grad','Some-college'],\n",
    "    \"Associate\" : ['Assoc-voc', 'Assoc-acdm'],\n",
    "    \"Senior\": ['Bachelors', 'Masters'],\n",
    "    \"Professional\": ['Doctorate', 'Prof-school']\n",
    "} \n",
    "def convertToEducationLevel(education):\n",
    "    for level, education in educationLevel.items() :\n",
    "        if level in education :\n",
    "            return level\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "def dataCleaningAndPreprocessing(dataset, cleanMissingData):\n",
    "    newDataSet = dataset\n",
    "    newDataSet=newDataSet.drop(['fnlwgt','education-num','relationship'],axis=1) #axis = 1 means drop column, axis = 0 => drop row\n",
    "    newDataSet['age'] = newDataSet['age'].apply(convertAge)\n",
    "    newDataSet['capital-gain'] = convertCapital(newDataSet['capital-gain'], newDataSet['capital-loss'])\n",
    "    newDataSet=newDataSet.drop('capital-loss',axis=1)\n",
    "    newDataSet['hours-per-week'] = combineHoursPerWeek(newDataSet['hours-per-week'])\n",
    "    if cleanMissingData == True: \n",
    "        newDataSet = newDataSet.loc[ (newDataSet['workclass'] != '?') & (newDataSet['occupation'] != '?') & (newDataSet['native-country']!= '?')]\n",
    "    else :\n",
    "        # newDataSet['native-country'] = newDataSet['native-country'].apply(handleUnknownNativeCountry)\n",
    "        # tread '?' as a group in workclass and occupation, because may have some reasons such as confidential, indescribable\n",
    "        newDataSet['area'] = newDataSet['native-country'].apply(countryToArea)\n",
    "        newDataSet = newDataSet.drop('native-country', axis=1)\n",
    "    # newDataSet['education'] = newDataSet['education'].apply(convertToEducationLevel)\n",
    "    return newDataSet\n",
    "\n",
    "def removeHeaders(headers):\n",
    "    headers.remove('fnlwgt') \n",
    "    headers.remove('education-num')\n",
    "    headers.remove('relationship')\n",
    "    headers.remove('capital-loss')\n",
    "    headers.remove('native-country')\n",
    "    return headers\n",
    "    \n",
    "updateAdult= dataCleaningAndPreprocessing(adult, False)\n",
    "updateAdultTest= dataCleaningAndPreprocessing(adultTest, False)\n",
    "headers = removeHeaders(headers)\n",
    "print(len(updateAdult.columns))\n",
    "\n",
    "########data-balancing for original data  ==> tested useless\n",
    "# from sklearn.utils import resample\n",
    "# majority = updateAdult[updateAdult.income == '<=50K'];\n",
    "# minority = updateAdult[updateAdult.income == '>50K'];\n",
    "# print(len(majority));\n",
    "# print(len(minority));\n",
    "\n",
    "# minority = resample(minority, replace=True, n_samples=len(majority))\n",
    "# updateAdult = pd.concat([majority, minority])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_text\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x_train = updateAdult.drop('income', axis=1)\n",
    "y_train = updateAdult['income']\n",
    "x_test = updateAdultTest.drop('income', axis=1)\n",
    "y_test = updateAdultTest['income']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "startTime = time.time()\n",
    "x_train_encoded = encoder.fit_transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "\n",
    "#get Best criterion, max_depth, min_samples_leaf, and min_samples_split\n",
    "postPruningStartTime = time.time()\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "path = dtc.cost_complexity_pruning_path(x_train_encoded, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "ccp_alphas = [alpha for alpha in ccp_alphas if alpha >= 0]\n",
    "ccp_alphas = list(set(ccp_alphas)) #remove duplicate values.\n",
    "print(f'get total number of {len(ccp_alphas)}')\n",
    "# Cross-validation for each alpha\n",
    "best_ccp_alpha = 0.0001\n",
    "scores = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dtc = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    score = np.median(cross_val_score(dtc, \n",
    "                                    x_train_encoded, \n",
    "                                    y_train,\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=0,\n",
    "                                    cv=5))\n",
    "    scores.append(score)\n",
    "best_ccp_alpha = ccp_alphas[np.argmax(scores)]\n",
    "print(\"Best ccp_alpha:\", best_ccp_alpha)\n",
    "ccpAlphaEndtime = time.time()\n",
    "timeSpent = ccpAlphaEndtime - postPruningStartTime\n",
    "print(f\"Post-Pruning Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "dtc = DecisionTreeClassifier(ccp_alpha=best_ccp_alpha)\n",
    "dtc.fit(x_train_encoded, y_train)\n",
    "# r = export_text(dtc, feature_names=headers)\n",
    "# print(r) #print tree\n",
    "trainEndtime = time.time()\n",
    "timeSpent = trainEndtime - startTime\n",
    "print(f\"Train Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "test_predictions_tree = dtc.predict(x_test_encoded)\n",
    "preditEndtime = time.time()\n",
    "timeSpent = preditEndtime - trainEndtime\n",
    "print(f\"Test Time spent: {timeSpent:.4f} seconds\")\n",
    "timeSpent = preditEndtime - startTime\n",
    "print(f\"Overall Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "train_predictions_tree = dtc.predict(x_train_encoded)\n",
    "train_accuracy_tree = accuracy_score(y_train, train_predictions_tree)\n",
    "test_accuracy_tree = accuracy_score(y_test, test_predictions_tree)\n",
    "print(f\"train accuracy:  {train_accuracy_tree:.4f}\")\n",
    "print(f\"test accuracy:  {test_accuracy_tree:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_test, test_predictions_tree)\n",
    "#cm[0][0] = TP\n",
    "#cm[1][1] = TN\n",
    "#cm[0][1] = FP\n",
    "#cm[1][0] = FN\n",
    "print(cm)\n",
    "print(classification_report(y_test, test_predictions_tree, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is DT with post-pruning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "get best other params {'max_depth': 15, 'min_samples_leaf': 9, 'min_samples_split': 5}\n",
      "Pre-Pruning Time spent: 7.1851 seconds\n",
      "Train Time spent: 7.2747 seconds\n",
      "Test Time spent: 0.0015 seconds\n",
      "Overall Time spent: 7.2761 seconds\n",
      "train accuracy:  0.8673\n",
      "test accuracy:  0.8479\n",
      "[[11469   966]\n",
      " [ 1510  2336]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8837    0.9223    0.9026     12435\n",
      "        >50K     0.7075    0.6074    0.6536      3846\n",
      "\n",
      "    accuracy                         0.8479     16281\n",
      "   macro avg     0.7956    0.7649    0.7781     16281\n",
      "weighted avg     0.8420    0.8479    0.8438     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "adult=pd.read_csv('adult.data', names=headers, skipinitialspace=True)\n",
    "adultTest=pd.read_csv('updateAdult.test', names=headers, skipinitialspace=True)\n",
    "\n",
    "def handleUnknownNativeCountry(country) :\n",
    "    if country == '?' :\n",
    "        return 'United-States'\n",
    "    else : \n",
    "        return country\n",
    "\n",
    "#data-preprocessing change continues to discrete\n",
    "#process age\n",
    "def convertAge(age):\n",
    "    if age < 22 :\n",
    "        return '<22'\n",
    "    elif age >=72 :\n",
    "        return '>=72'\n",
    "    else :\n",
    "        base_age = (age - 22) // 5\n",
    "        start_age = 22 + base_age * 5\n",
    "        end_age = 22 + 4 + base_age * 5\n",
    "        return f\"{start_age}-{end_age}\"\n",
    "\n",
    "#process capital-gain and capital-loss\n",
    "def convertCapital(capitalGain, capitalLoss):\n",
    "    dataLen = len(capitalGain)\n",
    "    if dataLen != len(capitalLoss):\n",
    "        raise AssertionError(\"Lengths of capitalGain and capitalLoss are not equal.\")\n",
    "    i = 0\n",
    "    capitalOptimized = []\n",
    "    while i < dataLen:\n",
    "        profit = capitalGain[i] - capitalLoss[i]\n",
    "        if(profit == 0):\n",
    "            capitalOptimized.append('No captial profit')\n",
    "        elif (profit < -3000):\n",
    "             capitalOptimized.append('<-3000')\n",
    "        elif (profit < 0):\n",
    "            capitalOptimized.append('>-3000&<0')\n",
    "        elif (profit <3000):\n",
    "            capitalOptimized.append('>0&<3000')\n",
    "        elif (profit < 6000):\n",
    "            capitalOptimized.append('>=3000&<6000')\n",
    "        else:\n",
    "            capitalOptimized.append('>=6000')\n",
    "        i = i + 1\n",
    "    return capitalOptimized\n",
    "\n",
    "# def combineHoursPerWeek(hoursPerWeek) :\n",
    "#     combineHoursPerWeekData = []\n",
    "#     for hoursPerWeekData in hoursPerWeek :\n",
    "#         hours = '=40'\n",
    "#         if hoursPerWeekData < 40 :\n",
    "#             hours = '<40'\n",
    "#         elif hoursPerWeekData > 40 :\n",
    "#             hours = '>40'\n",
    "#         combineHoursPerWeekData.append(hours)\n",
    "#     return combineHoursPerWeekData\n",
    "\n",
    "def combineHoursPerWeek(hoursPerWeek) :\n",
    "    combineHoursPerWeekData = []\n",
    "    for hoursPerWeekData in hoursPerWeek :\n",
    "        if hoursPerWeekData < 33:\n",
    "            hours = \"<33\"\n",
    "        elif hoursPerWeekData < 40 :\n",
    "            hours = '>=33 & <40'\n",
    "        elif hoursPerWeekData < 45 :\n",
    "            hours = \">=40 & <45\"\n",
    "        elif hoursPerWeekData < 52 :\n",
    "            hours = \">=45 & <52\"\n",
    "        else :\n",
    "            hours = '>=52'\n",
    "        combineHoursPerWeekData.append(hours)\n",
    "    return combineHoursPerWeekData\n",
    "\n",
    "countryGroups = {\n",
    "    \"Local\" : ['United-States','?', 'South'],\n",
    "    \"North America\": ['Canada', 'Outlying-US(Guam-USVI-etc)', 'Puerto-Rico', 'Mexico'],\n",
    "    \"Europe\": ['Holand-Netherlands', 'England', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Portugal', \n",
    "               'Germany', 'Greece', 'Italy', 'Poland', 'Hungary'],\n",
    "    \"Asia\": ['Laos', 'India', 'Philippines', 'Hong', 'Japan', 'Cambodia', 'China', 'Taiwan', 'Iran', 'Thailand', 'Vietnam'],\n",
    "    \"Other America\": ['Jamaica', 'Cuba', 'Dominican-Republic', 'Haiti', 'Trinadad&Tobago', 'Guatemala', 'Honduras', 'El-Salvador', 'Nicaragua','Ecuador', 'Columbia', 'Peru'],\n",
    "}\n",
    "def countryToArea(country):\n",
    "    for group, countries in countryGroups.items():\n",
    "        if country in countries:\n",
    "            return group\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "educationLevel = {\n",
    "    \"Primary\" : ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'],\n",
    "    \"Middle\" : ['HS-grad','Some-college'],\n",
    "    \"Associate\" : ['Assoc-voc', 'Assoc-acdm'],\n",
    "    \"Senior\": ['Bachelors', 'Masters'],\n",
    "    \"Professional\": ['Doctorate', 'Prof-school']\n",
    "} \n",
    "def convertToEducationLevel(education):\n",
    "    for level, education in educationLevel.items() :\n",
    "        if level in education :\n",
    "            return level\n",
    "    return \"Unknown\"  # no one will call this, but for in case\n",
    "\n",
    "def dataCleaningAndPreprocessing(dataset, cleanMissingData):\n",
    "    newDataSet = dataset\n",
    "    newDataSet=newDataSet.drop(['fnlwgt','education-num','relationship'],axis=1) #axis = 1 means drop column, axis = 0 => drop row\n",
    "    newDataSet['age'] = newDataSet['age'].apply(convertAge)\n",
    "    newDataSet['capital-gain'] = convertCapital(newDataSet['capital-gain'], newDataSet['capital-loss'])\n",
    "    newDataSet=newDataSet.drop('capital-loss',axis=1)\n",
    "    newDataSet['hours-per-week'] = combineHoursPerWeek(newDataSet['hours-per-week'])\n",
    "    if cleanMissingData == True: \n",
    "        newDataSet = newDataSet.loc[ (newDataSet['workclass'] != '?') & (newDataSet['occupation'] != '?') & (newDataSet['native-country']!= '?')]\n",
    "    else :\n",
    "        # newDataSet['native-country'] = newDataSet['native-country'].apply(handleUnknownNativeCountry)\n",
    "        # tread '?' as a group in workclass and occupation, because may have some reasons such as confidential, indescribable\n",
    "        newDataSet['area'] = newDataSet['native-country'].apply(countryToArea)\n",
    "        newDataSet = newDataSet.drop('native-country', axis=1)\n",
    "    # newDataSet['education'] = newDataSet['education'].apply(convertToEducationLevel)\n",
    "    return newDataSet\n",
    "\n",
    "def removeHeaders(headers):\n",
    "    headers.remove('fnlwgt') \n",
    "    headers.remove('education-num')\n",
    "    headers.remove('relationship')\n",
    "    headers.remove('capital-loss')\n",
    "    headers.remove('native-country')\n",
    "    return headers\n",
    "    \n",
    "updateAdult= dataCleaningAndPreprocessing(adult, False)\n",
    "updateAdultTest= dataCleaningAndPreprocessing(adultTest, False)\n",
    "headers = removeHeaders(headers)\n",
    "print(len(updateAdult.columns))\n",
    "\n",
    "########data-balancing for original data  ==> tested useless\n",
    "# from sklearn.utils import resample\n",
    "# majority = updateAdult[updateAdult.income == '<=50K'];\n",
    "# minority = updateAdult[updateAdult.income == '>50K'];\n",
    "# print(len(majority));\n",
    "# print(len(minority));\n",
    "\n",
    "# minority = resample(minority, replace=True, n_samples=len(majority))\n",
    "# updateAdult = pd.concat([majority, minority])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_text\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x_train = updateAdult.drop('income', axis=1)\n",
    "y_train = updateAdult['income']\n",
    "x_test = updateAdultTest.drop('income', axis=1)\n",
    "y_test = updateAdultTest['income']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "startTime = time.time()\n",
    "x_train_encoded = encoder.fit_transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "\n",
    "#get Best criterion, max_depth, min_samples_leaf, and min_samples_split\n",
    "prepruningStartTime = time.time()\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    # 'criterion': ['gini', \"entropy\"],\n",
    "    'max_depth': [8,9,10,11,13,15],\n",
    "    'min_samples_split': range(5,20,4), \n",
    "    'min_samples_leaf': range(3,10,1)\n",
    "}\n",
    "gridSearch = GridSearchCV(dtc, \n",
    "                           param_grid,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1,\n",
    "                           cv=5)\n",
    "gridSearch.fit(x_train_encoded, y_train)\n",
    "best_params = gridSearch.best_params_\n",
    "print(f'get best other params {best_params}')\n",
    "prepruningEndTime = time.time()\n",
    "timeSpent = prepruningEndTime - prepruningStartTime\n",
    "print(f\"Pre-Pruning Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "dtc = DecisionTreeClassifier(\n",
    "                                max_depth=best_params.get('max_depth'), \n",
    "                                min_samples_leaf=best_params.get('min_samples_leaf'), \n",
    "                                min_samples_split=best_params.get('min_samples_split'),\n",
    "                                # ccp_alpha=best_ccp_alpha\n",
    "                            )\n",
    "dtc.fit(x_train_encoded, y_train)\n",
    "# r = export_text(dtc, feature_names=headers)\n",
    "# print(r) #print tree\n",
    "trainEndtime = time.time()\n",
    "timeSpent = trainEndtime - startTime\n",
    "print(f\"Train Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "test_predictions_tree = dtc.predict(x_test_encoded)\n",
    "preditEndtime = time.time()\n",
    "timeSpent = preditEndtime - trainEndtime\n",
    "print(f\"Test Time spent: {timeSpent:.4f} seconds\")\n",
    "timeSpent = preditEndtime - startTime\n",
    "print(f\"Overall Time spent: {timeSpent:.4f} seconds\")\n",
    "\n",
    "train_predictions_tree = dtc.predict(x_train_encoded)\n",
    "train_accuracy_tree = accuracy_score(y_train, train_predictions_tree)\n",
    "test_accuracy_tree = accuracy_score(y_test, test_predictions_tree)\n",
    "print(f\"train accuracy:  {train_accuracy_tree:.4f}\")\n",
    "print(f\"test accuracy:  {test_accuracy_tree:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_test, test_predictions_tree)\n",
    "#cm[0][0] = TP\n",
    "#cm[1][1] = TN\n",
    "#cm[0][1] = FP\n",
    "#cm[1][0] = FN\n",
    "print(cm)\n",
    "print(classification_report(y_test, test_predictions_tree, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is DT with pre-pruning only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
